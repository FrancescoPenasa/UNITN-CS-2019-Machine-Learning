{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ass3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZRfrPNobi3g",
        "colab_type": "code",
        "outputId": "cc2bb132-ee45-40d2-b091-78e029340463",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "\"\"\"\n",
        "1. Build a neural network (at least 3 convolutional layers);\n",
        "2. Do model selection (optimizing hyperparameters or testing different\n",
        "architectures, performing validation by splitting the train set);\n",
        "3. Train your network over the full training set;\n",
        "4. Use the network to predict the examples in the test set;\n",
        "5. Place the labels in a file, in the same order as you read the test\n",
        "examples and in the same format of the labels in the training set\n",
        "\"\"\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n1. Build a neural network (at least 3 convolutional layers);\\n2. Do model selection (optimizing hyperparameters or testing different\\narchitectures, performing validation by splitting the train set);\\n3. Train your network over the full training set;\\n4. Use the network to predict the examples in the test set;\\n5. Place the labels in a file, in the same order as you read the test\\nexamples and in the same format of the labels in the training set\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgtSCaOY6lmN",
        "colab_type": "code",
        "outputId": "dd5ff7a7-b427-4f8b-84a8-11343e9e6592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "# mount the drive dir\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb5-A_X1cRXG",
        "colab_type": "code",
        "outputId": "04c5ea6d-6557-45ef-8bbf-97315779c5a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# import and init tensorflow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.random.set_seed(0)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fqX1L1PcjFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read train dataset and labels\n",
        "test_data = pd.read_csv('drive/My Drive/ocr/test-data.csv')\n",
        "test_target = pd.read_csv('drive/My Drive/ocr/test-target.csv')\n",
        "train_data = pd.read_csv('drive/My Drive/ocr/train-data.csv')\n",
        "train_target = pd.read_csv('drive/My Drive/ocr/train-target.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbrNjt70Tpsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# random split in train_data and train_target in 80% train, 20% validate\n",
        "train_data_sample = train_data.sample(frac=0.8, random_state=1)\n",
        "validate_data_sample = train_data.drop(train_data_sample.index)\n",
        "\n",
        "train_target_sample = train_target.sample(frac=0.8, random_state=1)\n",
        "validate_target_sample = train_target.drop(train_target_sample.index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AS5YBL2XCObf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reformat dataframe\n",
        "# data\n",
        "x_train = train_data_sample.values \n",
        "x_validate = validate_data_sample.values\n",
        "x_test = test_data.values\n",
        "x_train = x_train.reshape(len(train_data_sample), 16, 8)\n",
        "x_validate = x_validate.reshape(len(validate_data_sample), 16, 8)\n",
        "x_test = x_test.reshape(len(test_data), 16, 8)\n",
        "x_train = x_train[..., tf.newaxis]\n",
        "x_validate = x_validate[..., tf.newaxis]\n",
        "x_test = x_test[..., tf.newaxis]\n",
        "\n",
        "# targets\n",
        "one_hot_encode_train = pd.get_dummies(train_target_sample.iloc[:,0])\n",
        "y_train = one_hot_encode_train.values\n",
        "one_hot_encode_validate = pd.get_dummies(validate_target_sample.iloc[:,0])\n",
        "y_validate = one_hot_encode_validate.values\n",
        "one_hot_encode_test = pd.get_dummies(test_target.iloc[:,0])\n",
        "y_test = one_hot_encode_test.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAF3zz0qXFNL",
        "colab_type": "code",
        "outputId": "be4d7f66-cf7c-41b6-f8b3-9da59eb377c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "# sanity check\n",
        "print(x_train.shape)\n",
        "print(x_validate.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_validate.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(33376, 16, 8, 1)\n",
            "(8344, 16, 8, 1)\n",
            "(10430, 16, 8, 1)\n",
            "(33376, 26)\n",
            "(8344, 26)\n",
            "(10430, 26)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Pzk5VWc2osA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Softmax, Flatten"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFNVsnhl0zj8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MnistPerceptron(Model): # inherit from Model\n",
        "  def __init__(self):\n",
        "    super().__init__() # initialize Model\n",
        "    self.flatten = Flatten() # used to flatten pixels \n",
        "    self.W = tf.Variable(tf.zeros([128, 26]))# weights\n",
        "    self.b = tf.Variable(tf.zeros([26]))    # bias\n",
        "    self.softmax = Softmax()\n",
        "    \n",
        "\n",
        "  def call(self, x,training=False): \n",
        "    # the  training argument is unused in this model, we will need it later \n",
        "    x = self.flatten(x) # flatten images   \n",
        "\n",
        "    x = tf.dtypes.cast(x, tf.float32)\n",
        "    #self.W = tf.constant(self.W, dtype=tf.int64)\n",
        "    \n",
        "    multiplied = tf.matmul(x, self.W) # matmul, output shape : (batch, 10)\n",
        "    # we can equivalently do:\n",
        "    #multiplied = tf.transpose(tf.linalg.matmul(tf.float64(tf.transpose(self.W)), tf.float64(tf.transpose(flat))))\n",
        "\n",
        "    fwded = multiplied + self.b # broadcast self.b to (batch, 10) and add   \n",
        "\n",
        "    prob = self.softmax(fwded) # softmax              \n",
        "    return prob\n",
        "\n",
        "# Create an instance of the model\n",
        "perceptron = MnistPerceptron()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bac59va85alT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### TRAIN ###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FNu2Jmb3CoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss function\n",
        "perceptron_loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train, y_train)).shuffle(10000).batch(100)\n",
        "validate_ds = tf.data.Dataset.from_tensor_slices((x_validate, y_validate)).batch(50)  \n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(50)\n",
        "\n",
        "# choose an optimizer for the training\n",
        "perceptron_optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtCyIAGP6Eqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss_metric = tf.keras.metrics.Mean()\n",
        "train_accuracy_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "test_loss_metric = tf.keras.metrics.Mean()\n",
        "test_accuracy_metric = tf.keras.metrics.CategoricalAccuracy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmbzAsgQHiPA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_step(images, labels, model, loss_fn, optimizer):\n",
        "  with tf.GradientTape() as tape: # all the operations within this scope will be recorded in tape\n",
        "    predictions = model(images, training=True)\n",
        "    loss = loss_fn(labels, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  train_loss_metric(loss)\n",
        "  train_accuracy_metric(labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z82P0y88Hlo2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def train_loop(epochs, train_ds, model, loss_fn, optimizer):\n",
        "  for epoch in range(epochs):\n",
        "      # reset the metrics for the next epoch\n",
        "    train_loss_metric.reset_states()\n",
        "    train_accuracy_metric.reset_states()\n",
        "\n",
        "    start = datetime.now() # save start time \n",
        "    for images, labels in train_ds:\n",
        "      train_step(images, labels, model, loss_fn, optimizer)\n",
        "\n",
        "    template = 'Epoch {}, Time {}, Loss: {}, Accuracy: {}'\n",
        "    print(template.format(epoch+1,\n",
        "                          datetime.now() - start,\n",
        "                          train_loss_metric.result(),\n",
        "                          train_accuracy_metric.result()*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rgwTgFLHo65",
        "colab_type": "code",
        "outputId": "40669a9e-50c9-44a0-f2a0-990d31ccf66e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "train_loop(EPOCHS, train_ds, perceptron, perceptron_loss, perceptron_optimizer)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Time 0:00:04.516575, Loss: 2.8663129806518555, Accuracy: 32.873924255371094\n",
            "Epoch 2, Time 0:00:02.761206, Loss: 2.4397799968719482, Accuracy: 39.41455078125\n",
            "Epoch 3, Time 0:00:02.804536, Loss: 2.19722056388855, Accuracy: 42.923057556152344\n",
            "Epoch 4, Time 0:00:02.825475, Loss: 2.028562068939209, Accuracy: 47.1506462097168\n",
            "Epoch 5, Time 0:00:02.877635, Loss: 1.9030202627182007, Accuracy: 50.593238830566406\n",
            "Epoch 6, Time 0:00:02.790541, Loss: 1.8053250312805176, Accuracy: 53.10103225708008\n",
            "Epoch 7, Time 0:00:02.829558, Loss: 1.727095603942871, Accuracy: 55.28523254394531\n",
            "Epoch 8, Time 0:00:02.793644, Loss: 1.662144422531128, Accuracy: 56.79230499267578\n",
            "Epoch 9, Time 0:00:02.790353, Loss: 1.607544183731079, Accuracy: 58.04170227050781\n",
            "Epoch 10, Time 0:00:02.800229, Loss: 1.5609805583953857, Accuracy: 59.24616622924805\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGyEXOVmHqzS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### evaluation ###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuGTUsuE4CKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_step(images, labels, model, loss_fn):\n",
        "  predictions = model(images, training=False)\n",
        "  t_loss = loss_fn(labels, predictions)\n",
        "\n",
        "  test_loss_metric(t_loss)\n",
        "  test_accuracy_metric(labels, predictions)\n",
        "  return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-BYQOfV4EV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_loop(test_ds, model, loss_fn):\n",
        "  predictions = []\n",
        "\n",
        "  # reset the metrics for the next epoch\n",
        "  test_loss_metric.reset_states()\n",
        "  test_accuracy_metric.reset_states()\n",
        " \n",
        "  for test_images, test_labels in test_ds:\n",
        "    y = test_step(test_images, test_labels, model, loss_fn)\n",
        "    predictions.append(y)\n",
        "\n",
        "  template = 'Test Loss: {}, Test Accuracy: {}'\n",
        "  print(template.format(test_loss_metric.result(),\n",
        "                        test_accuracy_metric.result()*100))\n",
        "  return ([predictions, test_accuracy_metric.result()*100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDIqjwhx4Ff1",
        "colab_type": "code",
        "outputId": "d8e72eaf-df28-456d-cfaf-a4d853dda9f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tmp = test_loop(validate_ds, perceptron, perceptron_loss)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 1.5452115535736084, Test Accuracy: 59.86337661743164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfOZc6fS4HFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### deep architectures ###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCmQQCQn5F6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MnistConvolutional(Model):\n",
        "  def __init__(self, in_channels, out_channels, size):\n",
        "    super().__init__() # setup the model basic functionalities (mandatory)\n",
        "    initial = tf.random.truncated_normal([size, size, in_channels, out_channels], stddev=0.1)\n",
        "    self.filters = tf.Variable(initial) # create weights for the filters\n",
        "\n",
        "  def call(self, x):\n",
        "    x = tf.dtypes.cast(x, tf.float32)\n",
        "    res = tf.nn.conv2d(x, self.filters, 1, padding=\"SAME\")\n",
        "    return res\n",
        "\n",
        "class MnistFullyConnected(Model):\n",
        "  def __init__(self, input_shape, output_shape):\n",
        "    super().__init__() # initialize the model\n",
        "    self.W = tf.Variable(tf.random.truncated_normal([input_shape, output_shape], stddev=0.1)) # declare weights \n",
        "    self.b = tf.Variable(tf.constant(0.1, shape=[1, output_shape]))  # declare biases\n",
        "\n",
        "  def call(self, x):\n",
        "    res = tf.matmul(x, self.W) + self.b \n",
        "    #print (\"weight: \", self.W)\n",
        "    #print (\"bias: \", self.b)\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onukvaiI5Hi2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import MaxPool2D, Dropout\n",
        "\n",
        "class MnistDeepModel(Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()                          # 8,16,1   \n",
        "    \n",
        "    self.conv1 = MnistConvolutional(1, 16, 5)   # 8,16,16\n",
        "    self.pool1 = MaxPool2D([2,2])               # 4,8,16\n",
        "    self.conv2 = MnistConvolutional(16, 32, 5)  # 4,8,32\n",
        "    self.pool2 = MaxPool2D([2,2])               # 2,4,32\n",
        "    self.conv3 = MnistConvolutional(32, 48, 5)  # 2,4,48\n",
        "    self.pool3 = MaxPool2D([2,2])               # 1,2,48\n",
        "\n",
        "    self.flatten = Flatten()                  \n",
        "    self.fc1 = MnistFullyConnected(1*2*48, 1024) \n",
        "    self.dropout = Dropout(0.5)               \n",
        "    self.fc2 = MnistFullyConnected(1024, 26)  \n",
        "    self.softmax = Softmax()                  \n",
        "\n",
        "  def call(self, x, training=False):\n",
        "    \n",
        "    #print(\"conv1\")\n",
        "    x = tf.nn.relu(self.conv1(x))\n",
        "    x = self.pool1(x)\n",
        "\n",
        "    #print(\"conv2\")\n",
        "    x = tf.nn.relu(self.conv2(x))\n",
        "    x = self.pool2(x)\n",
        "\n",
        "    #print(\"conv3\")\n",
        "    x = tf.nn.relu(self.conv3(x))\n",
        "    x = self.pool3(x)\n",
        "\n",
        "    x = self.flatten(x)\n",
        "    x = tf.nn.relu(self.fc1(x))\n",
        "\n",
        "    x = self.dropout(x, training=training) # behavior of dropout changes between train and test\n",
        "    \n",
        "    x = self.fc2(x)\n",
        "    prob = self.softmax(x)\n",
        "    \n",
        "    return prob\n",
        "\n",
        "# Create an instance of the model\n",
        "network = MnistDeepModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHZ94sNt5JDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "network_loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "network_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4b377_F5KQ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "182318b9-9251-45b5-b7c6-ff783e730119"
      },
      "source": [
        "EPOCHS = 10\n",
        "train_loop(EPOCHS, train_ds,  network, network_loss, network_optimizer)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Time 0:00:09.418029, Loss: 2.465547561645508, Accuracy: 31.35486602783203\n",
            "Epoch 2, Time 0:00:04.780949, Loss: 1.3273367881774902, Accuracy: 61.562198638916016\n",
            "Epoch 3, Time 0:00:04.812601, Loss: 0.9714465141296387, Accuracy: 71.75814819335938\n",
            "Epoch 4, Time 0:00:04.871568, Loss: 0.8025233745574951, Accuracy: 76.23741912841797\n",
            "Epoch 5, Time 0:00:04.829216, Loss: 0.6999712586402893, Accuracy: 79.44031524658203\n",
            "Epoch 6, Time 0:00:04.790445, Loss: 0.6294887065887451, Accuracy: 81.2949447631836\n",
            "Epoch 7, Time 0:00:04.872173, Loss: 0.5797750949859619, Accuracy: 82.73609924316406\n",
            "Epoch 8, Time 0:00:04.821142, Loss: 0.5424246191978455, Accuracy: 83.65292358398438\n",
            "Epoch 9, Time 0:00:04.811452, Loss: 0.512635350227356, Accuracy: 84.56076049804688\n",
            "Epoch 10, Time 0:00:04.828199, Loss: 0.4837630093097687, Accuracy: 85.23789978027344\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XiZJUQt-5gs",
        "colab_type": "code",
        "outputId": "33822914-9355-4f92-84e9-534069e45d6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        }
      },
      "source": [
        "# loop between 1e-2, 1e-3, 1e-4 and find the best through the validate_sample\n",
        "EPOCHS = 10\n",
        "best_accuracy = 0\n",
        "best_learning_rate = 0\n",
        "learning_rates = [1e-2, 1e-3, 1e-4]\n",
        "for learning_rate in learning_rates:\n",
        "  network = MnistDeepModel()\n",
        "  network_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "  train_loop(EPOCHS, train_ds,  network, network_loss, network_optimizer)\n",
        "  tmp = test_loop(validate_ds, network, network_loss)\n",
        "  accuracy = tmp[1]\n",
        "  if (accuracy > best_accuracy):\n",
        "    best_accuracy = accuracy\n",
        "    best_learning_rate = learning_rate"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Time 0:00:05.060786, Loss: 0.8836331963539124, Accuracy: 74.14309692382812\n",
            "Epoch 2, Time 0:00:04.947687, Loss: 0.513607382774353, Accuracy: 84.84239959716797\n",
            "Epoch 3, Time 0:00:04.910434, Loss: 0.45549508929252625, Accuracy: 86.5472183227539\n",
            "Epoch 4, Time 0:00:04.962711, Loss: 0.46380290389060974, Accuracy: 86.56519317626953\n",
            "Epoch 5, Time 0:00:04.896364, Loss: 0.4494195878505707, Accuracy: 87.03559875488281\n",
            "Epoch 6, Time 0:00:04.906170, Loss: 0.44908809661865234, Accuracy: 87.0655517578125\n",
            "Epoch 7, Time 0:00:04.831342, Loss: 0.4475567638874054, Accuracy: 87.35318756103516\n",
            "Epoch 8, Time 0:00:04.877182, Loss: 0.43075424432754517, Accuracy: 87.80860137939453\n",
            "Epoch 9, Time 0:00:04.780779, Loss: 0.46129941940307617, Accuracy: 87.40711975097656\n",
            "Epoch 10, Time 0:00:04.872363, Loss: 0.4576101303100586, Accuracy: 87.29925537109375\n",
            "Test Loss: 0.44554996490478516, Test Accuracy: 87.69175720214844\n",
            "Epoch 1, Time 0:00:05.183386, Loss: 1.1076533794403076, Accuracy: 67.46764373779297\n",
            "Epoch 2, Time 0:00:05.159653, Loss: 0.4970511794090271, Accuracy: 84.76150512695312\n",
            "Epoch 3, Time 0:00:04.879544, Loss: 0.38670217990875244, Accuracy: 87.64981079101562\n",
            "Epoch 4, Time 0:00:04.741792, Loss: 0.33846092224121094, Accuracy: 89.16586303710938\n",
            "Epoch 5, Time 0:00:04.792007, Loss: 0.2945564091205597, Accuracy: 90.25347900390625\n",
            "Epoch 6, Time 0:00:04.906118, Loss: 0.2704341411590576, Accuracy: 91.00251770019531\n",
            "Epoch 7, Time 0:00:04.835432, Loss: 0.2421124428510666, Accuracy: 92.03619384765625\n",
            "Epoch 8, Time 0:00:04.965479, Loss: 0.2263288050889969, Accuracy: 92.42569732666016\n",
            "Epoch 9, Time 0:00:04.880847, Loss: 0.20411378145217896, Accuracy: 93.06388092041016\n",
            "Epoch 10, Time 0:00:04.870672, Loss: 0.19792479276657104, Accuracy: 93.144775390625\n",
            "Test Loss: 0.26655808091163635, Test Accuracy: 91.5987548828125\n",
            "Epoch 1, Time 0:00:04.863524, Loss: 2.4779889583587646, Accuracy: 29.527204513549805\n",
            "Epoch 2, Time 0:00:04.899249, Loss: 1.314172387123108, Accuracy: 61.6520881652832\n",
            "Epoch 3, Time 0:00:04.873148, Loss: 0.9735973477363586, Accuracy: 71.40760040283203\n",
            "Epoch 4, Time 0:00:04.922112, Loss: 0.8018919229507446, Accuracy: 76.17449951171875\n",
            "Epoch 5, Time 0:00:04.912428, Loss: 0.6971369981765747, Accuracy: 79.0328369140625\n",
            "Epoch 6, Time 0:00:04.908652, Loss: 0.6202044486999512, Accuracy: 81.2949447631836\n",
            "Epoch 7, Time 0:00:04.926644, Loss: 0.576941728591919, Accuracy: 82.64321899414062\n",
            "Epoch 8, Time 0:00:04.956622, Loss: 0.5339268445968628, Accuracy: 83.82670593261719\n",
            "Epoch 9, Time 0:00:04.947228, Loss: 0.5028302669525146, Accuracy: 84.6985855102539\n",
            "Epoch 10, Time 0:00:04.975066, Loss: 0.47488102316856384, Accuracy: 85.29182434082031\n",
            "Test Loss: 0.4275505244731903, Test Accuracy: 86.92473602294922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4C_x10xkMKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# take the whole training set\n",
        "x_train = train_data_sample.values\n",
        "x_train = x_train.reshape(len(train_data_sample), 16, 8)\n",
        "x_train = x_train[..., tf.newaxis]\n",
        "one_hot_encode_train = pd.get_dummies(train_target_sample.iloc[:,0])\n",
        "y_train = one_hot_encode_train.values\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train, y_train)).shuffle(10000).batch(100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWYWR1TXvR8O",
        "colab_type": "code",
        "outputId": "8ae0ea0e-8659-4c57-ca57-cfad8115d144",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "source": [
        "# train with the best_learning_rate\n",
        "network = MnistDeepModel()\n",
        "network_optimizer = tf.keras.optimizers.Adam(learning_rate=best_learning_rate)\n",
        "train_loop(EPOCHS, train_ds,  network, network_loss, network_optimizer)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Time 0:00:04.938634, Loss: 1.1317442655563354, Accuracy: 67.10211181640625\n",
            "Epoch 2, Time 0:00:04.902868, Loss: 0.5084572434425354, Accuracy: 84.50083923339844\n",
            "Epoch 3, Time 0:00:04.865430, Loss: 0.39394670724868774, Accuracy: 87.55992126464844\n",
            "Epoch 4, Time 0:00:04.947098, Loss: 0.3345670998096466, Accuracy: 89.30668640136719\n",
            "Epoch 5, Time 0:00:04.996534, Loss: 0.28956833481788635, Accuracy: 90.48717498779297\n",
            "Epoch 6, Time 0:00:04.936914, Loss: 0.26194432377815247, Accuracy: 91.30812072753906\n",
            "Epoch 7, Time 0:00:04.907091, Loss: 0.24254223704338074, Accuracy: 91.9463119506836\n",
            "Epoch 8, Time 0:00:04.870573, Loss: 0.2191823571920395, Accuracy: 92.6174545288086\n",
            "Epoch 9, Time 0:00:04.902683, Loss: 0.20302428305149078, Accuracy: 93.06986999511719\n",
            "Epoch 10, Time 0:00:04.931508, Loss: 0.18772661685943604, Accuracy: 93.4264144897461\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7J_-746vpLW",
        "colab_type": "code",
        "outputId": "496b1620-30f8-47aa-87bb-4763c4af9ba9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# test with the best_learning_rate\n",
        "y_pred = test_loop(test_ds, network, network_loss)\n",
        "y_pred = y_pred[0]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.31525084376335144, Test Accuracy: 90.2684555053711\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZAvZvzP6E17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reconvert in label the predictions\n",
        "predictions_labeled = ['g'] # list of predictions, init with the first one\n",
        "converter = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
        "for i in range(0, len(y_pred)):\n",
        "  for j in range(0, len(y_pred[i])):\n",
        "    prediction = np.argmax(y_pred[i][j,:])\n",
        "    predictions_labeled.append(converter[prediction])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmDMD4NCYbBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# export the predictions\n",
        "data = pd.DataFrame(predictions_labeled)\n",
        "data.to_csv(\"data.csv\", index=False, header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1G1r32HyAAHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}